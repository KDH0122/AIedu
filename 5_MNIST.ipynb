{"cells":[{"cell_type":"markdown","metadata":{"id":"DpaX8L1-TlrP"},"source":["# Softmax Classification"]},{"cell_type":"markdown","metadata":{"id":"pbYCMJY7Tpdd"},"source":["# 0. Softmax를 이용한 MNIST 기본 구현"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blW0defxRJHt"},"outputs":[],"source":["# 0. Softmax를 이용한 MNIST 기본 구현\n","\n","import tensorflow as tf\n","import random\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.1\n","training_cnt = 15\n","batch_size = 100\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","W = tf.Variable(tf.random.normal([784, 10]))\n","b = tf.Variable(tf.random.normal([10]))\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            logits = tf.matmul(batch_xs,W) + b\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        W_grad, b_grad = tape.gradient(cost, [W,b])\n","        W.assign_sub(learning_rate*W_grad)\n","        b.assign_sub(learning_rate*b_grad)\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","with tf.GradientTape() as tape:\n","    logits = tf.matmul(x_test,W) + b\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"M587ln2ER-Zp"},"source":["#### seed 값 설정\n","- 재현성을 위해 tf.random.set_seed 함수를 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O27iJk0nSRY8"},"outputs":[],"source":["tf.random.set_seed(1234)"]},{"cell_type":"markdown","metadata":{"id":"pRLFTg7kSfGZ"},"source":["#### 파라메터 값 설정\n","- learning rate: weight가 발산되지 않도록 조정하는 값으로 weight값이 너무 작으면 train이 되지 않을 수 있고, 너무 크면 overshooting이 발생할 수 있다\n","- training_cnt: 전체 데이터셋에 대한 학습 반복 횟수(Epoch)\n","- batch_size: 한번에 학습할 데이터의 수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYAwDpoYSiVg"},"outputs":[],"source":["learning_rate = 0.1\n","training_cnt = 15\n","batch_size = 100"]},{"cell_type":"markdown","metadata":{"id":"z1MSIgfbSlMf"},"source":["#### tf.random.normal\n","- 784개의 픽셀마다 가중치들을 각각 학습하여 0부터 9까지 숫자를 인식\n","- weight, bias의 초기값을 난수로 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCEY7CsKThsl"},"outputs":[],"source":["W = tf.Variable(tf.random.normal([784, 10]))\n","b = tf.Variable(tf.random.normal([10]))"]},{"cell_type":"markdown","metadata":{"id":"tWwhYWELUFx7"},"source":["#### matmul 함수 사용\n","- 입력 X와 가중치 W를 곱하고 편향 b를 더하여 모델을 정의한다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVm21pNvUOme"},"outputs":[],"source":["logits = tf.matmul(batch_xs,W) + b"]},{"cell_type":"markdown","metadata":{"id":"bsEpacpFUtwo"},"source":["#### cost/loss function 구현\n","- 교차 엔트로피(cross-entropy) 사용\n","- 예측값과 실제값 사이의 확률분포 차이 계산\n","- 학습 방법으로 GradientDescent 함수 사용 (경사하강법)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhkicuV9VEU9"},"outputs":[],"source":["with tf.GradientTape() as tape:\n","    logits = tf.matmul(batch_xs,W) + b\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","W_grad, b_grad = tape.gradient(cost, [W,b])\n","W.assign_sub(learning_rate*W_grad)\n","b.assign_sub(learning_rate*b_grad)"]},{"cell_type":"markdown","metadata":{"id":"jetHuo6ZpYdI"},"source":["#### 학습된 예측값 확인, 정확도 계산\n","- softmax 함수를 적용하여 출력값의 합이 항상 1이 되게 한다\n","- 예측된 최대값의 index 반환\n","- One-hot encoding한 Y값도 최대값 1이 있는 index 반환\n","- 평균을 이용하여 예측값과 실제 데이터의 일치 여부를 계산"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hPeCHOUvq9Ix"},"outputs":[],"source":["pred = tf.nn.softmax(logits)\n","prediction = tf.argmax(pred, 1)\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))"]},{"cell_type":"markdown","metadata":{"id":"H0fZbWAErZa-"},"source":["#### 모델 실행\n","- Tensorflow에서 제공하는 훈련 데이터(train data)가 60000개이기 때문에 여러 개의 batch로 나누어 학습을 진행하는 것이 효율적이다\n","- total_batch는 60000/100 = 600이다\n","- training_cnt만큼 반복하는 for문 안에 total_batch만큼 반복하는 for문이 포함되어 있다\n","- avg_cost는 전체 cost를 total_batch만큼 나눈 값을 더하여 계산된다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcyuL-3KrOEN"},"outputs":[],"source":["for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            logits = tf.matmul(batch_xs,W) + b\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        W_grad, b_grad = tape.gradient(cost, [W,b])\n","        W.assign_sub(learning_rate*W_grad)\n","        b.assign_sub(learning_rate*b_grad)\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')"]},{"cell_type":"markdown","metadata":{"id":"PAmMqZN6uMOc"},"source":["- 전체 데이터 셋을 반복한 단계와 각 단계에 해당하는 cost인 avg_cost를 출력\n","- Accuracy는 훈련 데이터(Train data)로 학습한 모델을 시험 데이터(Test data)를 대상으로 적용한 정확도를 나타낸다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmHTCvReukjH"},"outputs":[],"source":["print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')"]},{"cell_type":"markdown","metadata":{"id":"xV1W4w3PvA_X"},"source":["#### 모델 검정(test)\n","- Accuracy는 학습한 모델로 훈련 데이터(Train data)와 시험 데이터(Test data)를 대상으로 적용한 정확도를 나타낸다\n","- r은 시험데이터(Test data)에서 랜덤하게 1개를 읽어 온 것이다\n","- Label은 r에 해당하는 0~9사이의 실제 레이블(Label)\n","- Prediction은 r에 해당하는 이미지의 예측값(0~9사이의 레이블)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXmO1dM8uvNW"},"outputs":[],"source":["print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","with tf.GradientTape() as tape:\n","    logits = tf.matmul(x_test,W) + b\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"jePh9U414wNV"},"source":["# 1. Deep Neural Network와 ReLU를 추가하여 모델 변경\n","- 심층신경망(Deep Neural Network) 구성과 ReLU함수 사용\n","- 이전 softmax를 이용한 구현과 다른 점은 여러 개의 layer를 추가하여 심층신경망을 구성한 것과 활성화 함수로 ReLU함수를 사용한 것이다\n","- 각 layer의 결과값이 다음 layer의 입력값으로 연결되는 것을 주목한다\n","- 동일한 결과값을 위해 seed 옵션을 설정한다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWeYbVjCyz9K"},"outputs":[],"source":["# 1. Deep Neural Network와 ReLU를 추가하여 모델 변경\n","\n","import tensorflow as tf\n","import random\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.1\n","training_cnt = 15\n","batch_size = 100\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","# deep neural network와 ReLU 추가\n","W1 = tf.Variable(tf.random.normal([784, 256]))\n","b1 = tf.Variable(tf.random.normal([256]))\n","\n","W2 = tf.Variable(tf.random.normal([256, 256]))\n","b2 = tf.Variable(tf.random.normal([256]))\n","\n","W3 = tf.Variable(tf.random.normal([256, 10]))\n","b3 = tf.Variable(tf.random.normal([10]))\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            logits = tf.matmul(L2, W3) + b3\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,b1,b2,b3]\n","        grads = tape.gradient(cost, weight_list)\n","        for i in range(len(weight_list)):\n","            weight_list[i].assign_sub(learning_rate*grads[i])\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    logits = tf.matmul(L2, W3) + b3\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"cS04wQwJsadb"},"source":["# 2. Learning Rate 조정\n","- Cost가 기존 대비 큰 값(2.xx)을 가지며 더 이상 0에 가깝게 수렴하지 않는다\n","- Overshooting의 가능성이 있으므로 learning_rate 값을 줄여본다\n","- learning_rate 값은 10의 누승으로 다양하게 변경해본다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Uu2I1bVsZXr"},"outputs":[],"source":["# 2. Learning Rate 조정\n","\n","import tensorflow as tf\n","import random\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.01\n","training_cnt = 15\n","batch_size = 100\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","W1 = tf.Variable(tf.random.normal([784, 256]))\n","b1 = tf.Variable(tf.random.normal([256]))\n","\n","W2 = tf.Variable(tf.random.normal([256, 256]))\n","b2 = tf.Variable(tf.random.normal([256]))\n","\n","W3 = tf.Variable(tf.random.normal([256, 10]))\n","b3 = tf.Variable(tf.random.normal([10]))\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            logits = tf.matmul(L2, W3) + b3\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,b1,b2,b3] \n","        grads = tape.gradient(cost, weight_list)\n","        for i in range(len(weight_list)):\n","            weight_list[i].assign_sub(learning_rate*grads[i])\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    logits = tf.matmul(L2, W3) + b3\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"p8F1gAfcs7vH"},"source":["# 3. Adam Optimizer 적용 (향상된 Optimizer 사용)\n","- 딥러닝 Optimizer 중 성능이 좋은 것으로 평가되는 Adam Optimizer를 사용한다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Bpuy06Hs7cX"},"outputs":[],"source":["# 3. Adam Optimizer 적용\n","\n","import tensorflow as tf\n","import random\n","\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.01\n","training_cnt = 15\n","batch_size = 100\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","W1 = tf.Variable(tf.random.normal([784, 256]))\n","b1 = tf.Variable(tf.random.normal([256]))\n","\n","W2 = tf.Variable(tf.random.normal([256, 256]))\n","b2 = tf.Variable(tf.random.normal([256]))\n","\n","W3 = tf.Variable(tf.random.normal([256, 10]))\n","b3 = tf.Variable(tf.random.normal([10]))\n","\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            logits = tf.matmul(L2, W3) + b3\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,b1,b2,b3]\n","        grads = tape.gradient(cost, weight_list)\n","        opt.apply_gradients(zip(grads, weight_list))\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    logits = tf.matmul(L2, W3) + b3\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"YgcHg0MAwLJb"},"source":["# 4. Xavier Initializer 적용 (적절한 Weight Initializer 사용)\n","- 정확도를 높이기 위해 Xavier Initializer를 사용하여 가중치를 초기화 한다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eR-PiDxevGHt"},"outputs":[],"source":["# 4. Xavier Initializer 적용\n","\n","import tensorflow as tf\n","import random\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.01\n","training_cnt = 15\n","batch_size = 100\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","initializer = tf.initializers.GlorotUniform() # Xavier Initializer\n","W1 = tf.Variable(initializer(shape=(784, 256)))\n","\n","b1 = tf.Variable(tf.random.normal([256]))\n","\n","W2 = tf.Variable(initializer(shape=(256, 256)))\n","b2 = tf.Variable(tf.random.normal([256]))\n","\n","\n","W3 = tf.Variable(initializer(shape=(256, 10)))\n","b3 = tf.Variable(tf.random.normal([10]))\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            logits = tf.matmul(L2, W3) + b3\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,b1,b2,b3]\n","        grads = tape.gradient(cost, weight_list)\n","        opt.apply_gradients(zip(grads, weight_list))\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    logits = tf.matmul(L2, W3) + b3\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"PRL7eYBmNvXs"},"source":["# 5. learning_rate 재조정(0.01 -> 0.001)\n","- Weight 초기화 변경 이후 기존 대비 cost가 더 작은 값을 가지지만 학습 결과는 오히려 나빠졌다\n","- learning_rate를 변경해서 더 세밀하게 최적값을 찾도록 시도해본다\n","- learning_rate 값은 10의 누승으로 다양하게 변경해본다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDAOAxVS16Li"},"outputs":[],"source":["# 5. learning_rate 재조정(0.01 -> 0.001)\n","\n","import tensorflow as tf\n","import random\n","\n","mnist = tf.keras.datasets.mnist\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.001\n","training_cnt = 15\n","batch_size = 100\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","initializer = tf.initializers.GlorotUniform() # Xavier Initializer\n","W1 = tf.Variable(initializer(shape=(784, 256)))\n","\n","b1 = tf.Variable(tf.random.normal([256]))\n","\n","W2 = tf.Variable(initializer(shape=(256, 256)))\n","b2 = tf.Variable(tf.random.normal([256]))\n","\n","\n","W3 = tf.Variable(initializer(shape=(256, 10)))\n","b3 = tf.Variable(tf.random.normal([10]))\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            logits = tf.matmul(L2, W3) + b3\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,b1,b2,b3]\n","        grads = tape.gradient(cost, weight_list)\n","        opt.apply_gradients(zip(grads, weight_list))\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        true_Y = tf.argmax(batch_ys, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    logits = tf.matmul(L2, W3) + b3\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"RNT7R85-OkOT"},"source":["# 6. He Initializer 적용\n","- Xavier Initializer 대신 He Initializer를 사용하여 가중치를 초기화 한다\n","- 활성화 함수로 ReLU를 사용하는 경우 He Initializer를 사용하는게 효과적이다\n","- 이 예제처럼 모형이 복잡하지 않은 경우에 사용해야 효과가 좋다\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xibZ4TOOl0G"},"outputs":[],"source":["# 6. He Initializer 적용\n","\n","import tensorflow as tf\n","import random\n","\n","mnist = tf.keras.datasets.mnist\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.001\n","training_cnt = 15\n","batch_size = 100\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","initializer = tf.keras.initializers.HeNormal() # He Initializer\n","\n","W1 = tf.Variable(initializer(shape=(784, 256)))\n","\n","b1 = tf.Variable(tf.random.normal([256]))\n","\n","W2 = tf.Variable(initializer(shape=(256, 256)))\n","b2 = tf.Variable(tf.random.normal([256]))\n","\n","\n","W3 = tf.Variable(initializer(shape=(256, 10)))\n","b3 = tf.Variable(tf.random.normal([10]))\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            logits = tf.matmul(L2, W3) + b3\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,b1,b2,b3]\n","        grads = tape.gradient(cost, weight_list)\n","        opt.apply_gradients(zip(grads, weight_list))\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","  \n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    logits = tf.matmul(L2, W3) + b3\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())"]},{"cell_type":"markdown","metadata":{"id":"94l-n6AvPPT3"},"source":["# 7. Deep & Wide Neural Network 확장\n","- 은닉층 노드 수를 증가시키고 레이어를 추가 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZ_f-I8pPZuk"},"outputs":[],"source":["# 7. Deep & Wide Neural Network 확장\n","\n","import tensorflow as tf\n","import random\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.001\n","training_cnt = 15\n","batch_size = 100\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","initializer = tf.initializers.HeNormal()\n","\n","W1 = tf.Variable(initializer(shape=(784, 512)))\n","b1 = tf.Variable(tf.random.normal([512]))\n","\n","W2 = tf.Variable(initializer(shape=(512, 512)))\n","b2 = tf.Variable(tf.random.normal([512]))\n","\n","\n","W3 = tf.Variable(initializer(shape=(512, 512)))\n","b3 = tf.Variable(tf.random.normal([512]))\n","\n","W4 = tf.Variable(initializer(shape=(512,512)))\n","b4 = tf.Variable(tf.random.normal([512]))\n","\n","W5 = tf.Variable(initializer(shape=(512,10)))\n","b5 = tf.Variable(tf.random.normal([10]))\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n","            L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n","            logits = tf.matmul(L4, W5) + b5\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,W4,W5,b1,b2,b3,b4,b5]\n","        grads = tape.gradient(cost, weight_list)\n","        opt.apply_gradients(zip(grads, weight_list))\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n","    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n","    logits = tf.matmul(L4, W5) + b5\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())\n"]},{"cell_type":"markdown","metadata":{"id":"zJSck-qzT4c_"},"source":["# 8. Dropout 적용\n","- Dropout은 Overfitting이 일어나지 않도록 중간 중간 무작위로 뉴런을 비활성화하여 성능을 향상시키는 방법이다\n","- 학습 시간은 다소 길어지지만 모델의 일반적인 예측 성능을 높여준다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfIEx4J5S2Dh"},"outputs":[],"source":["# 8. Dropout 적용\n","\n","import tensorflow as tf\n","import random\n","\n","tf.random.set_seed(1234)\n","\n","learning_rate = 0.001\n","training_cnt = 15\n","batch_size = 100\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train, x_test = x_train.astype('float32').reshape(60000,784), x_test.astype('float32').reshape(10000,784)\n","y_train, y_test = tf.one_hot(y_train,10), tf.one_hot(y_test,10) \n","\n","tf.random.set_seed(1234)\n","\n","rate = 0.7\n","\n","initializer = tf.initializers.HeNormal()\n","\n","W1 = tf.Variable(initializer(shape=(784, 512)))\n","b1 = tf.Variable(tf.random.normal([512]))\n","\n","W2 = tf.Variable(initializer(shape=(512, 512)))\n","b2 = tf.Variable(tf.random.normal([512]))\n","\n","\n","W3 = tf.Variable(initializer(shape=(512, 512)))\n","b3 = tf.Variable(tf.random.normal([512]))\n","\n","W4 = tf.Variable(initializer(shape=(512,512)))\n","b4 = tf.Variable(tf.random.normal([512]))\n","\n","W5 = tf.Variable(initializer(shape=(512,10)))\n","b5 = tf.Variable(tf.random.normal([10]))\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","for epoch in range(training_cnt):\n","    total_batch = int(len(x_train) / batch_size)\n","    avg_cost = 0\n","    correct_prediction = []\n","    for i in range(total_batch): \n","        start = batch_size * i\n","        end = batch_size*(1+i)\n","        batch_xs, batch_ys = x_train[start:end], y_train[start:end]\n","        with tf.GradientTape() as tape:\n","            L1 = tf.nn.relu(tf.matmul(batch_xs, W1) + b1)\n","            L1 = tf.nn.dropout(L1, rate=rate)\n","            L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","            L2 = tf.nn.dropout(L2, rate=rate)\n","            L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n","            L3 = tf.nn.dropout(L3, rate=rate)\n","            L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n","            L4 = tf.nn.dropout(L4, rate=rate)\n","            logits = tf.matmul(L4, W5) + b5\n","            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch_ys))\n","        weight_list = [W1,W2,W3,W4,W5,b1,b2,b3,b4,b5]\n","        grads = tape.gradient(cost, weight_list)\n","        opt.apply_gradients(zip(grads, weight_list))\n","        pred = tf.nn.softmax(logits)\n","        prediction = tf.argmax(pred, 1)\n","        avg_cost += cost.numpy() / total_batch\n","        correct_prediction = tf.concat([correct_prediction,prediction],0)\n","    \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","true_Y = tf.argmax(y_train, 1)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_prediction, true_Y), dtype=tf.float32))\n","print('Accuracy(train):','{:.3f}'.format(accuracy)) # train set에 대한 accuracy\n","\n","\n","with tf.GradientTape() as tape:\n","    L1 = tf.nn.relu(tf.matmul(x_test, W1) + b1)\n","    L1 = tf.nn.dropout(L1, rate=rate)\n","    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","    L2 = tf.nn.dropout(L2, rate=rate)\n","    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n","    L3 = tf.nn.dropout(L3, rate=rate)\n","    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n","    L4 = tf.nn.dropout(L4, rate=rate)\n","    logits = tf.matmul(L4, W5) + b5\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(y_test, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","\n","print('Accuracy(test):','{:.3f}'.format(accuracy)) # test set에 대한 accuracy\n","\n","r = random.randint(0, len(x_test) - 1)\n","print(\"Label: \", tf.argmax(y_test[r:r + 1], 1).numpy())\n","print(\"Prediction: \", prediction[r:r+1].numpy())\n"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}