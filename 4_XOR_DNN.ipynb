{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# XOR_DNN"],"metadata":{"id":"yuTLtz66yzhJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mDk6oo2u45M"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","tf.random.set_seed(777)\n","\n","learning_rate = 0.1\n","training_cnt = 10000\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n","Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n","\n","# Layer 1\n","W1 = tf.Variable(tf.random.normal([2, 2]), name='weight1',dtype=tf.float32,trainable=True)\n","b1 = tf.Variable(tf.random.normal([2]), name='bias1', dtype=tf.float32, trainable=True)\n","\n","# Layer 2\n","W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2', dtype=tf.float32, trainable=True)\n","b2 = tf.Variable(tf.random.normal([1]), name='bias2', dtype=tf.float32, trainable=True)\n","\n","\n","for step in range(training_cnt):\n","    with tf.GradientTape() as tape:\n","        L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","        pred = tf.sigmoid(tf.matmul(L1, W2) + b2)\n","        cost = -tf.reduce_mean(Y * tf.math.log(pred) + (1 - Y) * tf.math.log(1-pred))\n","    weight_list = [W1,W2,b1,b2]\n","    W1_grad, W2_grad, b1_grad, b2_grad = tape.gradient(cost, weight_list)\n","    W1.assign_sub(learning_rate*W1_grad)\n","    W2.assign_sub(learning_rate*W2_grad)\n","    b1.assign_sub(learning_rate*b1_grad)\n","    b2.assign_sub(learning_rate*b2_grad)\n","    predicted = tf.cast(pred > 0.5, dtype=tf.float32)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))  \n","    if step % 1000 == 0:\n","        print(step, cost.numpy(), W1.numpy(), W2.numpy(), b1.numpy(), b2.numpy())\n","\n","print(\"\\nPred: \", pred.numpy(), \"\\nPredicted: \", predicted.numpy(), \"\\nAccuracy: \", accuracy.numpy())"]},{"cell_type":"markdown","source":["#### tf.random.set_seed\n","- 랜덤하게 생성되는 숫자들을 동일하게 생성하기 위한 것으로 실습 결과 비교를 위해 실행한다 "],"metadata":{"id":"5tf-JrfLozlS"}},{"cell_type":"code","source":["tf.random.set_seed(777)"],"metadata":{"id":"EdiuYvygo3-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 파라메터 값 설정\n","- 학습을 위한 기초 파라메터\n","- learning_rate : 값이 너무 적으면 Train 되지 않을 수 있고 값이 너무 크면 overshooting이 발생할 수 있다\n","- training_cnt : dataset에 대한 training 반복 횟수"],"metadata":{"id":"rbQM8EiNpDOx"}},{"cell_type":"code","source":["learning_rate = 0.1\n","training_cnt = 10000"],"metadata":{"id":"hPht-mCepZ7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 트레이닝 데이터 변수 선언\n","- 입력으로 들어가는 X(input 2개), Y(output 1개) 설정\n","- numpy array를 사용"],"metadata":{"id":"GCXiczetpfZd"}},{"cell_type":"code","source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n","Y = np.array([[0], [1], [1], [0]], dtype=np.float32)"],"metadata":{"id":"qxQePSmlp6AJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### tf.random.normal\n","- bias, weight의 초기값을 난수로 생성"],"metadata":{"id":"3tv3fxaAp9Iq"}},{"cell_type":"code","source":["W1 = tf.Variable(tf.random.normal([2, 2]), name='weight1',dtype=tf.float32,trainable=True)\n","b1 = tf.Variable(tf.random.normal([2]), name='bias1', dtype=tf.float32, trainable=True)"],"metadata":{"id":"73gBzV6eqFMQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### sigmoid 함수 사용\n","- Y값이 0 또는 1의 binary 값을 갖기 때문에 logistic regression을 활용하고 sigmoid 함수를 사용한다"],"metadata":{"id":"dXpTWvOFqJRh"}},{"cell_type":"code","source":["L1 = tf.sigmoid(tf.matmul(X, W1) + b1)"],"metadata":{"id":"b9hUbKW-qVfx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 은닉층(hidden layer) 만들기\n","- \"Layer 2\"에선 첫 번째 레이어의 출력값인 \"L1\" 입력값이 된다\n","- 마지막 레이어에선 Weight 와 bias 값이 1개가 되도록 설정하고 sigmoid 함수를 사용하여 모델을 정의한다"],"metadata":{"id":"PQ4oO5d5qZbo"}},{"cell_type":"code","source":["W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2', dtype=tf.float32, trainable=True)\n","b2 = tf.Variable(tf.random.normal([1]), name='bias2', dtype=tf.float32, trainable=True)\n","pred = tf.sigmoid(tf.matmul(L1, W2) + b2)"],"metadata":{"id":"G92eJJqIqyg3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### cost/loss function 구현\n","- 0~1 사이의 값을 근사화 하기 위해서 log함수를 사용\n","$$C(H(x),y) = \\frac{1}{m}∑(-y\\log(H(x))-(1-y)\\log(1-H(x))$$"],"metadata":{"id":"Rjwiyw4Vq5rV"}},{"cell_type":"code","source":["cost = -tf.reduce_mean(Y * tf.math.log(pred) + (1 - Y) * tf.math.log(1-pred))"],"metadata":{"id":"haEO7Eh9rIvs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 학습 방법 -> cost를 최소화 \n","- GradientDescent 함수 사용 (경사하강법)"],"metadata":{"id":"QBAOuPL-tG2J"}},{"cell_type":"code","source":["with tf.GradientTape() as tape:\n","    L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","    pred = tf.sigmoid(tf.matmul(L1, W2) + b2)\n","    cost = -tf.reduce_mean(Y * tf.math.log(pred) + (1 - Y) * tf.math.log(1-pred))\n","weight_list = [W1,W2,b1,b2]\n","W1_grad, W2_grad, b1_grad, b2_grad = tape.gradient(cost, weight_list)\n","W1.assign_sub(learning_rate*W1_grad)\n","W2.assign_sub(learning_rate*W2_grad)\n","b1.assign_sub(learning_rate*b1_grad)\n","b2.assign_sub(learning_rate*b2_grad)"],"metadata":{"id":"Y7x1LGJNuHoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 학습된 예측값을 0과 1로 변환\n","- 0~1 사이로 학습된 예측값을 0과 1로 나누어 분류"],"metadata":{"id":"1zV9WNEguRs8"}},{"cell_type":"code","source":["predicted = tf.cast(pred > 0.5, dtype=tf.float32)"],"metadata":{"id":"pIrfrm8LuRFi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 정확도 \n","- accuracy를 계산하여 분류가 정확한지 확인\n","- 예측값과 실제 데이터의 일치 여부 계산\n","- 아래 코드는 평균을 이용한 정확도 계산 "],"metadata":{"id":"RMN0AHDWufmW"}},{"cell_type":"code","source":["accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"],"metadata":{"id":"pw56r9rtuvlk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 모델실행 \n","- pred는 sigmoid 함수를 통해 0~1 사이의 값으로 나온다\n","- predicted는 pred에서 나온 값을 0과 1로 변환 시킨 값이다\n","- accuracy는 '예측한 Y값과 실제 Y값과 얼마나 일치하는가'이다. "],"metadata":{"id":"3AREBj-iu2L_"}},{"cell_type":"code","source":["for step in range(training_cnt):\n","    with tf.GradientTape() as tape:\n","        L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","        pred = tf.sigmoid(tf.matmul(L1, W2) + b2)\n","        cost = -tf.reduce_mean(Y * tf.math.log(pred) + (1 - Y) * tf.math.log(1-pred))\n","    weight_list = [W1,W2,b1,b2]\n","    W1_grad, W2_grad, b1_grad, b2_grad = tape.gradient(cost, weight_list)\n","    W1.assign_sub(learning_rate*W1_grad)\n","    W2.assign_sub(learning_rate*W2_grad)\n","    b1.assign_sub(learning_rate*b1_grad)\n","    b2.assign_sub(learning_rate*b2_grad)\n","    predicted = tf.cast(pred > 0.5, dtype=tf.float32)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))  \n","    if step % 1000 == 0:\n","        print(step, cost.numpy(), W1.numpy(), W2.numpy(), b1.numpy(), b2.numpy())\n","\n","print(\"\\nPred: \", pred.numpy(), \"\\nPredicted: \", predicted.numpy(), \"\\nAccuracy: \", accuracy.numpy())"],"metadata":{"id":"85JhFQAbvMPL"},"execution_count":null,"outputs":[]}]}