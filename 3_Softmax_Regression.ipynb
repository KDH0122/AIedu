{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Softmax Classification"],"metadata":{"id":"208Qkgubd6WA"}},{"cell_type":"markdown","source":["## 1. 전체 학습코드"],"metadata":{"id":"4E5yEigvKDSf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tB4W6HmPdvR9"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","learning_rate = 0.01\n","training_cnt = 1000\n","display_step = 250\n","\n","X = np.array([[2, 3, 1, 1], [4, 3, 2, 1], [4, 2, 5, 5], [2, 5, 2, 1], [6, 3, 1, 2], [5, 4, 4, 2] ], dtype=np.float32)\n","Y = np.array([[0, 0, 1], [0, 0 ,1], [0, 1, 0], [0, 1, 0], [1, 0, 0 ], [1, 0 ,0]])\n","\n","W = tf.Variable(tf.random.normal([4, 3]), name='weight') \n","b = tf.Variable(tf.random.normal([3]), name='bias')  \n","\n","for epoch in range(training_cnt):\n","    with tf.GradientTape() as tape:\n","      pred = tf.nn.softmax(tf.matmul(X ,W) + b)\n","      cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))\n","    W_grad, b_grad = tape.gradient(cost, [W, b])\n","    W.assign_sub(learning_rate * W_grad)\n","    b.assign_sub(learning_rate * b_grad)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(Y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","    if (epoch+1) % display_step == 0:\n","        print('\\n*****************running****************\\n')\n","        print('Run_count :[{}] cost : [{:0.4f}] \\npred : {} \\npred_Y : {} \\ntrue_Y : {} \\naccuracy : {:.2%}% \\\n","              ' .format( (epoch+1), cost, pred, prediction, true_Y, accuracy)\n","             )\n","print(\"Optimization Finished!\") "]},{"cell_type":"markdown","source":["### 1) 파라메터 값 설정\n","- 머신러닝을 위한 기초 파라메터\n","- learning_rate : 값이 너무 적으면 Train 되지 않을 수 있고, 값이 너무 크면 overshooting이 발생할 수 있다.\n","- training_cnt : data set에 대한 training 반복 횟수\n"],"metadata":{"id":"4n_LIn7sgAXd"}},{"cell_type":"code","source":["# 파라메터값 설정\n","learning_rate = 0.01\n","training_cnt = 1000\n","display_step = 250  # 원하는 출력 위치 조정"],"metadata":{"id":"pf2taqNCcwJC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 트레이닝 데이터 변수 선언\n","- 입력으로 들어가는 x data(input 4개), y data(output 3개) 설정\n","- 레이블 데이터를 one-hot encoding 형태로 구성"],"metadata":{"id":"rWTWgaEZg1RS"}},{"cell_type":"code","source":["X = np.array([[2, 3, 1, 1], [4, 3, 2, 1], [4, 2, 5, 5], [2, 5, 2, 1], [6, 3, 1, 2], [5, 4, 4, 2] ], dtype=np.float32)\n","Y = np.array([[0, 0, 1], [0, 0 ,1], [0, 1, 0], [0, 1, 0], [1, 0, 0 ], [1, 0 ,0]])"],"metadata":{"id":"WPSKavZog0TF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3) tf.random.normal\n","- bias, weight 초기값을 난수로 생성"],"metadata":{"id":"_gyQXl29g_I9"}},{"cell_type":"code","source":["W = tf.Variable(tf.random.normal([4, 3]), name='weight')\n","b = tf.Variable(tf.random.normal([3]), name='bias')"],"metadata":{"id":"nZTT4bwQgdJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4) softmax 함수 사용\n","- 기존 pred 계산에 softmax 함수를 적용하여 출력값의 합이 항상 1이 되게 한다."],"metadata":{"id":"xttKfjo3ha_z"}},{"cell_type":"code","source":["pred = tf.nn.softmax(tf.matmul(X ,W) + b)"],"metadata":{"id":"Z6y1CoQ4hcjP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5) cost function\n","- 교차 엔트로피(cross-entropy) 사용\n","- 예측값과 실제값 사이의 확률분포 차이 계산"],"metadata":{"id":"x9vGezd5hfSW"}},{"cell_type":"code","source":["cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))"],"metadata":{"id":"crWnJH5Qhejw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6) 학습\n","- gradient decent 함수 사용(경사 하강법) \n","-  파라미터 $W$, $b$ 에 대해 손실을 미분하는 과정으로, 파라미터를 증가시킬 때 손실이 얼마나 변화하는지를 알아본다. \n","- with tf.GradientTape() as tape: 안에서 계산을 하면 tape에 계산 과정을 기록해두었다가 tape.gradient를 이용해서 미분을 자동으로 구할 수 있다"],"metadata":{"id":"qdtBqqWjhv2v"}},{"cell_type":"code","source":["with tf.GradientTape() as tape:\n","    pred = tf.nn.softmax(tf.matmul(X ,W) + b)\n","    cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))\n","W_grad, b_grad = tape.gradient(cost, [W, b])"],"metadata":{"id":"hr3To06WKg8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- $w←w−η∂w$ 의 식으로 파라미터를 수정\n","- $η$ 는 학습률\n","- 경사(미분)을 따라 손실을 줄여나가기 때문에 경사하강법이라고 부름\n","- a.assign_sub(b)는 a = a - b 와 같다"],"metadata":{"id":"pcb8bNEZQ43s"}},{"cell_type":"code","source":["W.assign_sub(learning_rate * W_grad)\n","b.assign_sub(learning_rate * b_grad)"],"metadata":{"id":"1adf3vd9hwnd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7) 학습된 예측 값을 확인\n","- 예측된 최대 값의 index 반환\n","- One-hot encoding 한 Y값도 최대값 1이 있는 index 반환"],"metadata":{"id":"HfkFWOXMiDMc"}},{"cell_type":"code","source":["prediction = tf.argmax(pred, 1)\n","true_Y = tf.argmax(Y, 1)"],"metadata":{"id":"FlDq_m9Yh_WU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8) 정확도\n","- accuracy를 계산하여 분류가 정확한지 확인\n","- 예측값과 실제 데이터의 일치 여부 계산 \n","- 아래 코드는 평균을 이용한 정확도 계산"],"metadata":{"id":"6aUuhStAiNm-"}},{"cell_type":"code","source":["accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))"],"metadata":{"id":"yOQ4qPbIiLk6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (2) 모델 실행(run/update)\n","- pred는 softmax 함수를 통해 0~1사이의 값으로 나온다\n","- pred_Y는 pred에서 나온 최대 예측값의 index 반환\n","- accuracy는 예측한 Y값이 실제 Y값과 얼마나 일치하는가"],"metadata":{"id":"fRmQg_XjiURK"}},{"cell_type":"code","source":["for epoch in range(training_cnt):\n","    with tf.GradientTape() as tape:\n","        pred = tf.nn.softmax(tf.matmul(X ,W) + b)\n","        cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))\n","    W_grad, b_grad = tape.gradient(cost, [W, b])\n","    W.assign_sub(learning_rate * W_grad)\n","    b.assign_sub(learning_rate * b_grad)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(Y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","    if (epoch+1) % display_step == 0:\n","        print('\\n*****************running****************\\n')\n","        print('Run_count :[{}] cost : [{:0.4f}] \\npred : {} \\npred_Y : {} \\ntrue_Y : {} \\naccuracy : {:.2%}% \\\n","              ' .format( (epoch+1), cost, pred, prediction, true_Y, accuracy)\n","             )\n","print(\"Optimization Finished!\") "],"metadata":{"id":"qIDWPyZsiUsE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. 기타 학습 방법\n","- 출력 Y를 one_hot 함수를 이용하여 encoding 한 후 학습\n","- one_hot으로 늘어난 차원을 reshape로 축소\n","- softmax_cross_entropy_with_logits 함수 이용"],"metadata":{"id":"yLm3s431ij_n"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","learning_rate = 0.01\n","training_cnt = 1000\n","display_step = 250\n","\n","X = np.array([[2, 3, 1, 1], [4, 3, 2, 1], [4, 2, 5, 5], [2, 5, 2, 1], [6, 3, 1, 2], [5, 4, 4, 2] ], dtype = np.float32)\n","Y = np.array([[2], [2], [1], [1], [0], [0]])\n","\n","Y_1 = tf.one_hot(Y, 3)  \n","Y_2 = tf.reshape(Y_1, [-1, 3]) \n","\n","W = tf.Variable(tf.random.normal([4, 3]), name='weight') \n","b = tf.Variable(tf.random.normal([3]), name='bias')  \n","\n","\n","with tf.GradientTape() as tape:\n","    logits = tf.matmul(X ,W) + b\n","    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_2)\n","    cost = tf.reduce_mean(cost_i)\n","\n","W_grad, b_grad = tape.gradient(cost, [W, b])\n","W.assign_sub(learning_rate * W_grad)\n","b.assign_sub(learning_rate * b_grad)\n","\n","print('Y : {} \\none_hot_Y: {} \\none_hot_reshape_Y {} ' .format(Y, Y_1,Y_2))\n","\n","for epoch in range(training_cnt):\n","    with tf.GradientTape() as tape:\n","        logits = tf.matmul(X ,W) + b\n","        cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_2)\n","        cost = tf.reduce_mean(cost_i)\n","    W_grad, b_grad = tape.gradient(cost, [W, b])\n","    W.assign_sub(learning_rate * W_grad)\n","    b.assign_sub(learning_rate * b_grad)\n","    pred = tf.nn.softmax(logits)\n","    prediction = tf.argmax(pred, 1)\n","    true_Y = tf.argmax(Y_2, 1)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n","    \n","    if (epoch+1) % display_step == 0:\n","        print('\\n*****************running****************\\n')\n","        print('Run_count :[{}] cost : [{:0.4f}] \\npred : {} \\npred_Y : {} \\ntrue_Y : {} \\naccuracy : {:.2%}% \\\n","              ' .format( (epoch+1), cost, pred, prediction, true_Y, accuracy)\n","             )\n","      \n","     \n","\n","print(\"Optimization Finished!\") "],"metadata":{"id":"Yb3pQZcLikr0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1) 정수로들어온 클래스 one_hot encoding 하기\n","- 0,1,2,3,.... 의 정수 클래스를 one_hot 함수 사용하여 변경\n","- one_hot을 하면 차원이 +1 이 된다.\n","- reshape를 통해 차원을 다시 맞춰준다."],"metadata":{"id":"jMfPnca2mv1e"}},{"cell_type":"code","source":["Y = np.array([[2], [2], [1], [1], [0], [0]], dtype=np.float32)\n","\n","Y_1 = tf.one_hot(Y, 3)  \n","Y_2 = tf.reshape(Y_1, [-1, 3]) "],"metadata":{"id":"LCBX7ZGHlTM-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2) cost 함수 사용\n","- softmax_cross_entropy_with_logits_v2 함수 이용\n","- softmax_cross_entropy_with_logits 함수의 업그레이드 버전\n","- one_hot encoding한 라벨값 사용"],"metadata":{"id":"QZBTJNOuoBnF"}},{"cell_type":"code","source":["logits = tf.matmul(X ,W) + b\n","cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_2)\n","cost = tf.reduce_mean(cost_i)"],"metadata":{"id":"SNu9h6Qmm3_l"},"execution_count":null,"outputs":[]}]}